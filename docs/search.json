[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jackson Marsh",
    "section": "",
    "text": "I am a current senior at St. Olaf College. I am majoring in Biology with a concentration in statistics and data science."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Mini_Project_1",
    "section": "",
    "text": "Show the code\nlibrary(tidyverse) #Load in packages \n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nShow the code\nlibrary(mdsr)\nlibrary(viridis)\n\n\nLoading required package: viridisLite\n\n\nShow the code\nlibrary(lubridate)\nlibrary(statebins)\nlibrary(leaflet)\nlibrary(sf)\n\n\nLinking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\n\nShow the code\nlibrary(htmltools)\nlibrary(glue)\nlibrary(maps)\n\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:viridis':\n\n    unemp\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n\nShow the code\nus_states &lt;- map_data(\"state\") #Load in data for the geometry of states\nhead(us_states)\n\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\nShow the code\nX1_self_reported &lt;- read_csv(\"https://www.cdc.gov/physical-activity/files/1-self-reported.csv\")\n\n\nRows: 53 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): State, Prevalence, 95% Confidence Interval\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n#Load in data\nPopular_Sports_Sheet1 &lt;- read_csv(\"https://raw.githubusercontent.com/JMarsh2467/SDS264_2024/refs/heads/main/Popular%20Sports%20-%20Sheet1.csv?token=GHSAT0AAAAAACZ5N4TKHY6EMPQF4MYQLJAIZZIGLFQ\")\n\n\nRows: 51 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): State, Sport\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\nActive_Data &lt;- X1_self_reported %&gt;% filter(State != \"District of Columbia\", State != \"Puerto Rico\", State != \"Guam\") #Removes areas that are not states\n\nActive_Data$State &lt;- tolower(Active_Data$State) #Changes format to be able to join\nActive_Data$Prevalence &lt;- na_if(Active_Data$Prevalence, \"Insufficient data**\")\n\nFull_Active_Data &lt;- Active_Data %&gt;%\n  right_join(us_states, by = c(\"State\" = \"region\")) #Join togther activity data and states geometry\n\nFull_Active_Data$Prevalence &lt;- as.numeric(Full_Active_Data$Prevalence) #Change formating \nstates &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")\nclass(states) #Read in additional state geometry\n\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nShow the code\nstates\n\n\nSimple feature collection with 52 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -188.9049 ymin: 17.92956 xmax: -65.6268 ymax: 71.35163\nGeodetic CRS:  WGS 84\n# A tibble: 52 × 4\n   id    name                  density                                  geometry\n   &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;                        &lt;MULTIPOLYGON [°]&gt;\n 1 01    Alabama                 94.6  (((-87.3593 35.00118, -85.60667 34.98475…\n 2 02    Alaska                   1.26 (((-131.602 55.11798, -131.5692 55.28229…\n 3 04    Arizona                 57.0  (((-109.0425 37.00026, -109.048 31.33163…\n 4 05    Arkansas                56.4  (((-94.47384 36.50186, -90.15254 36.4963…\n 5 06    California             242.   (((-123.2333 42.00619, -122.3789 42.0116…\n 6 08    Colorado                49.3  (((-107.9197 41.00391, -105.729 40.99843…\n 7 09    Connecticut            739.   (((-73.05353 42.03905, -71.79931 42.0226…\n 8 10    Delaware               464.   (((-75.41409 39.80446, -75.5072 39.68396…\n 9 11    District of Columbia 10065    (((-77.03526 38.99387, -76.90929 38.8952…\n10 12    Florida                353.   (((-85.49714 30.99754, -85.00421 31.0030…\n# ℹ 42 more rows\n\n\nShow the code\nstates$name &lt;- tolower(states$name) #changes formating to help join later \nstatesactive &lt;- states %&gt;% right_join(Active_Data, by = c(\"name\" = \"State\")) #joins togther state geometry with activity data \nstatesactive$Prevalence &lt;- as.numeric(statesactive$Prevalence) #changes prevalence to a numeric variable\nstate_plotting_sf &lt;- statesactive |&gt;\n  mutate(Prev_intervals = cut(Prevalence, n = 6,\n          breaks = c(15, 18, 21, 24, 27, 30, Inf))) |&gt; #Makes intervals fo the plot\n  filter(!(name %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\"))) #removes areas that are not states\n\nggplot(data = state_plotting_sf) + #makes plot\n  geom_sf(aes(fill = Prev_intervals), colour = \"white\", linetype = 2) +\n  theme_void() +  \n  scale_fill_brewer(palette = \"YlGnBu\")\n\n\n\n\n\n\n\n\n\nShow the code\nbins &lt;- c(15, 18, 21, 24, 27, 30, Inf) #adds bins for the plot \npal &lt;- colorBin(\"YlGnBu\", domain = states$Prevalence, bins = bins) \n\n\nWarning: Unknown or uninitialised column: `Prevalence`.\n\n\nShow the code\nstatesactive &lt;- statesactive |&gt;\n  mutate(labels = str_c(name, \": \", Prevalence, \"% of residents inactive\")) #creates labeles for plot \n\nlabels &lt;- lapply(statesactive $labels, HTML)"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jackson Marsh",
    "section": "Education",
    "text": "Education\nAustin High School: 2017-2021\nSt. Olaf College: 2021-2025"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Jackson Marsh",
    "section": "Experience",
    "text": "Experience\nNonin Medical Statistics Intern: 2024"
  },
  {
    "objectID": "about2.html",
    "href": "about2.html",
    "title": "Mini_Project_2",
    "section": "",
    "text": "Show the code\n#Initializing Used Libraries \nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(polite)\nlibrary(rvest)\nlibrary(httr)\nlibrary(xml2)\nlibrary(SentimentAnalysis)\n\n\n\nget_song_lyrics_genius &lt;- function(song, artist){\n  #Format song into the format Genius expects\n  song_url &lt;- song |&gt; \n    str_to_lower() |&gt; \n    str_remove_all(\"[^\\\\w|\\\\s]\") |&gt; \n    str_replace_all(\"\\\\s\", \"-\") \n    \n  #Format artist into the format Genius expects\n  artist_url &lt;- artist |&gt; \n    str_to_lower() |&gt; \n    str_remove_all(\" featuring [\\\\D]+\") |&gt; \n    str_replace_all(\" \", \"-\") |&gt; \n    str_replace_all(\"&\", \"and\")\n  \n  #Create full Genius url \n  full_url &lt;- str_c(\"https://genius.com/\",\n                    artist_url, \n                    \"-\", \n                    song_url, \n                    \"-lyrics\" \n                    )\n  \n  #Initialize web scrapping session\n  session &lt;- bow(full_url, force = TRUE)\n  \n    #Scrape the page\n    page &lt;- scrape(session)\n    if(is.null(page)){\n      warning(\"Invalid URL Entered: \", full_url, call. = T, immediate. = T)\n      tibble(\n        song,\n        artist, \n        lyrics = NA\n      )\n    }\n    else{\n    \n      #Change all &lt;br&gt; tags into spaces \n      #(why does html_text not do this automatically!?)\n      xml_find_all(page, \".//br\") |&gt; \n        xml_add_sibling(\"p\", \" \")\n      \n      xml_find_all(page, \".//br\") |&gt; \n        xml_remove()\n      \n      #Extract Lyrics\n      lyrics &lt;- html_nodes(page, \".Lyrics__Container-sc-1ynbvzw-1\") |&gt; \n        html_text() |&gt; \n        str_replace_all(\"\\\\[[^\\\\]]+\\\\]\", \" \") |&gt; \n        paste(collapse = \"\")\n      \n      #Create Output tibble\n      tibble(\n        song, \n        artist, \n        lyrics\n      )\n    }\n}\n\n\nget_billboard_hot_100 &lt;- function(){\n  #Initialing Webscraping Session\n  session &lt;- bow(\"https://www.billboard.com/charts/hot-100/\", force = TRUE)\n  \n  #Vector for placements on the Billboard Hot 100\n  placement &lt;- (1:100)\n  \n  #Getting song titles from Billboard Hot 100\n  song_title &lt;- scrape(session) |&gt; \n    html_nodes(\"li.o-chart-results-list__item &gt; h3#title-of-a-story\") |&gt;\n    html_text() |&gt; \n    str_trim() #Removing white space from html_text\n  \n  #Getting artist from Billboard Hot 100\n  artist &lt;- scrape(session) |&gt; \n    html_nodes(\"li.o-chart-results-list__item &gt; span.c-label.a-no-trucate\") |&gt; \n    html_text() |&gt; \n    str_trim() #Removing white space from html_text\n  \n  #Getting how many weeks a song has been on the Billboard Hot 100\n  weeks_on_chart &lt;- scrape(session) |&gt; \n    html_nodes(\"ul.lrv-a-unstyle-list.lrv-u-flex.lrv-u-height-100p li:nth-child(6) &gt; span.c-label\") |&gt; \n    html_text() |&gt; \n    str_trim() #Removing white space from html_text\n  \n  #Getting the Week that this chart is describing \n  week_of_chart &lt;- scrape(session) |&gt;\n    html_nodes(\"p.c-tagline.a-font-primary-medium-xs\") |&gt; \n    html_text() |&gt; \n    str_extract(\"Week of (\\\\D+ \\\\d+, \\\\d{4})\", 1) |&gt; #RegEx to extract the week\n    mdy()\n  \n  #Combining into one tibble\n  tibble(\n    placement, \n    song_title,\n    artist, \n    weeks_on_chart,\n    week_of_chart,\n  )\n}\n\n\n\nShow the code\ntop_100 &lt;- get_billboard_hot_100()\n\ntop_100_lyrics &lt;- map2(top_100$song_title[1:25],\n                       top_100$artist[1:25],\n                       \\(x, y) get_song_lyrics_genius(x, y)\n                       ) |&gt; \n  bind_rows()\n\n\n\n\nShow the code\nTop_100_Wordcount &lt;- SentimentAnalysis::analyzeSentiment(top_100_lyrics$lyrics) %&gt;% select(\"WordCount\")\nTop_100_Sentiment &lt;- SentimentAnalysis::analyzeSentiment(top_100_lyrics$lyrics) %&gt;% select(\"SentimentGI\")\n\n\n\n\nShow the code\ntop_100_lyrics$WordCount &lt;- Top_100_Wordcount\ntop_100_lyrics$SentimentGI &lt;- Top_100_Sentiment\n\n\n\n\nShow the code\nprint(top_100_lyrics)\n\n\n# A tibble: 25 × 5\n   song               artist   lyrics WordCount$WordCount SentimentGI$Sentimen…¹\n   &lt;chr&gt;              &lt;chr&gt;    &lt;chr&gt;                &lt;dbl&gt;                  &lt;dbl&gt;\n 1 Love Somebody      Morgan … \"  Ru…                 154                 0.221 \n 2 A Bar Song (Tipsy) Shabooz… \"  My…                 230                -0.1   \n 3 Birds Of A Feather Billie … \"    …                 144                 0.0417\n 4 Die With A Smile   Lady Ga… \"  (O…                 128                 0.0859\n 5 Espresso           Sabrina… \"  No…                 200                 0.135 \n 6 I Had Some Help    Post Ma… \"  Yo…                 158                 0.0127\n 7 Lose Control       Teddy S… \"  So…                 136                -0.0294\n 8 APT.               ROSE & … \"  채…                 215                 0.0744\n 9 Taste              Sabrina… \"  Oh…                 151                 0.245 \n10 Beautiful Things   Benson … \"  Fo…                 133                 0.105 \n# ℹ 15 more rows\n# ℹ abbreviated name: ¹​SentimentGI$SentimentGI"
  },
  {
    "objectID": "about.html#showcasing-a-project-i-did-making-interactive-maps",
    "href": "about.html#showcasing-a-project-i-did-making-interactive-maps",
    "title": "Mini_Project_1",
    "section": "",
    "text": "Show the code\nlibrary(tidyverse) #Load in packages \n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nShow the code\nlibrary(mdsr)\nlibrary(viridis)\n\n\nLoading required package: viridisLite\n\n\nShow the code\nlibrary(lubridate)\nlibrary(statebins)\nlibrary(leaflet)\nlibrary(sf)\n\n\nLinking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\n\nShow the code\nlibrary(htmltools)\nlibrary(glue)\nlibrary(maps)\n\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:viridis':\n\n    unemp\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n\nShow the code\nus_states &lt;- map_data(\"state\") #Load in data for the geometry of states\nhead(us_states)\n\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\nShow the code\nX1_self_reported &lt;- read_csv(\"https://www.cdc.gov/physical-activity/files/1-self-reported.csv\")\n\n\nRows: 53 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): State, Prevalence, 95% Confidence Interval\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n#Load in data\nPopular_Sports_Sheet1 &lt;- read_csv(\"https://raw.githubusercontent.com/JMarsh2467/SDS264_2024/refs/heads/main/Popular%20Sports%20-%20Sheet1.csv?token=GHSAT0AAAAAACZ5N4TKHY6EMPQF4MYQLJAIZZIGLFQ\")\n\n\nRows: 51 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): State, Sport\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\nActive_Data &lt;- X1_self_reported %&gt;% filter(State != \"District of Columbia\", State != \"Puerto Rico\", State != \"Guam\") #Removes areas that are not states\n\nActive_Data$State &lt;- tolower(Active_Data$State) #Changes format to be able to join\nActive_Data$Prevalence &lt;- na_if(Active_Data$Prevalence, \"Insufficient data**\")\n\nFull_Active_Data &lt;- Active_Data %&gt;%\n  right_join(us_states, by = c(\"State\" = \"region\")) #Join togther activity data and states geometry\n\nFull_Active_Data$Prevalence &lt;- as.numeric(Full_Active_Data$Prevalence) #Change formating \nstates &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")\nclass(states) #Read in additional state geometry\n\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nShow the code\nstates\n\n\nSimple feature collection with 52 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -188.9049 ymin: 17.92956 xmax: -65.6268 ymax: 71.35163\nGeodetic CRS:  WGS 84\n# A tibble: 52 × 4\n   id    name                  density                                  geometry\n   &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;                        &lt;MULTIPOLYGON [°]&gt;\n 1 01    Alabama                 94.6  (((-87.3593 35.00118, -85.60667 34.98475…\n 2 02    Alaska                   1.26 (((-131.602 55.11798, -131.5692 55.28229…\n 3 04    Arizona                 57.0  (((-109.0425 37.00026, -109.048 31.33163…\n 4 05    Arkansas                56.4  (((-94.47384 36.50186, -90.15254 36.4963…\n 5 06    California             242.   (((-123.2333 42.00619, -122.3789 42.0116…\n 6 08    Colorado                49.3  (((-107.9197 41.00391, -105.729 40.99843…\n 7 09    Connecticut            739.   (((-73.05353 42.03905, -71.79931 42.0226…\n 8 10    Delaware               464.   (((-75.41409 39.80446, -75.5072 39.68396…\n 9 11    District of Columbia 10065    (((-77.03526 38.99387, -76.90929 38.8952…\n10 12    Florida                353.   (((-85.49714 30.99754, -85.00421 31.0030…\n# ℹ 42 more rows\n\n\nShow the code\nstates$name &lt;- tolower(states$name) #changes formating to help join later \nstatesactive &lt;- states %&gt;% right_join(Active_Data, by = c(\"name\" = \"State\")) #joins togther state geometry with activity data \nstatesactive$Prevalence &lt;- as.numeric(statesactive$Prevalence) #changes prevalence to a numeric variable\nstate_plotting_sf &lt;- statesactive |&gt;\n  mutate(Prev_intervals = cut(Prevalence, n = 6,\n          breaks = c(15, 18, 21, 24, 27, 30, Inf))) |&gt; #Makes intervals fo the plot\n  filter(!(name %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\"))) #removes areas that are not states\n\nggplot(data = state_plotting_sf) + #makes plot\n  geom_sf(aes(fill = Prev_intervals), colour = \"white\", linetype = 2) +\n  theme_void() +  \n  scale_fill_brewer(palette = \"YlGnBu\")\n\n\n\n\n\n\n\n\n\nShow the code\nbins &lt;- c(15, 18, 21, 24, 27, 30, Inf) #adds bins for the plot \npal &lt;- colorBin(\"YlGnBu\", domain = states$Prevalence, bins = bins) \n\n\nWarning: Unknown or uninitialised column: `Prevalence`.\n\n\nShow the code\nstatesactive &lt;- statesactive |&gt;\n  mutate(labels = str_c(name, \": \", Prevalence, \"% of residents inactive\")) #creates labeles for plot \n\nlabels &lt;- lapply(statesactive $labels, HTML)"
  },
  {
    "objectID": "about.html#map-showcasing-the-activity-levels-of-states",
    "href": "about.html#map-showcasing-the-activity-levels-of-states",
    "title": "Mini_Project_1",
    "section": "Map showcasing the activity levels of states",
    "text": "Map showcasing the activity levels of states\n\n\nShow the code\nleaflet(statesactive) %&gt;%\n  setView(-96, 37.8, 4) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal(Prevalence), #Tells how we will color the plot \n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) %&gt;%\n  addLegend(pal = pal, values = ~density, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\n\nWarning in sf::st_is_longlat(x): bounding box has potentially an invalid value\nrange for longlat data\n\n\n\n\n\n\n\n\nShow the code\nPopular_Sports &lt;- Popular_Sports_Sheet1\n\nPopular_Sports$State &lt;- tolower(Popular_Sports$State) #change formating\n\nFull_Popular_Sports &lt;- Popular_Sports %&gt;%\n  right_join(us_states, by = c(\"State\" = \"region\")) #joining sport data with geometry \nstatessports &lt;- states %&gt;% right_join(Popular_Sports, by = c(\"name\" = \"State\")) #joining data \nstate_plotting_sf &lt;- statessports |&gt;\n  filter(!(name %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\"))) #removes non-states\n\nggplot(data = state_plotting_sf) + \n  geom_sf(aes(fill = Sport), colour = \"white\", linetype = 2) + \n  theme_void() +  \n  scale_fill_brewer(palette = \"Set3\")\n\n\n\n\n\n\n\n\n\nShow the code\npal &lt;- colorFactor(\"Set2\", states$Prevalence)\n\n\nWarning: Unknown or uninitialised column: `Prevalence`.\n\n\nShow the code\nstatessports &lt;- statessports |&gt;\n  mutate(labels = str_c(name, \": \", Sport)) #sets labels \n\nlabels &lt;- lapply(statessports$labels, HTML)"
  },
  {
    "objectID": "about.html#map-showcasing-the-most-watched-sport-in-each-state",
    "href": "about.html#map-showcasing-the-most-watched-sport-in-each-state",
    "title": "Mini_Project_1",
    "section": "Map showcasing the most watched sport in each state",
    "text": "Map showcasing the most watched sport in each state\n\n\nShow the code\nleaflet(statessports) %&gt;%\n  setView(-96, 37.8, 4) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal(statessports$Sport), #Tells how we will color plot\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\"))\n\n\nWarning in sf::st_is_longlat(x): bounding box has potentially an invalid value\nrange for longlat data"
  },
  {
    "objectID": "about2.html#showcasing-a-project-i-did-web-scraping-data-for-the-lyrics-of-popular-songs",
    "href": "about2.html#showcasing-a-project-i-did-web-scraping-data-for-the-lyrics-of-popular-songs",
    "title": "Mini_Project_2",
    "section": "",
    "text": "Show the code\n#Initializing Used Libraries \nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(polite)\nlibrary(rvest)\nlibrary(httr)\nlibrary(xml2)\nlibrary(SentimentAnalysis)\n\n\n\nget_song_lyrics_genius &lt;- function(song, artist){\n  #Format song into the format Genius expects\n  song_url &lt;- song |&gt; \n    str_to_lower() |&gt; \n    str_remove_all(\"[^\\\\w|\\\\s]\") |&gt; \n    str_replace_all(\"\\\\s\", \"-\") \n    \n  #Format artist into the format Genius expects\n  artist_url &lt;- artist |&gt; \n    str_to_lower() |&gt; \n    str_remove_all(\" featuring [\\\\D]+\") |&gt; \n    str_replace_all(\" \", \"-\") |&gt; \n    str_replace_all(\"&\", \"and\")\n  \n  #Create full Genius url \n  full_url &lt;- str_c(\"https://genius.com/\",\n                    artist_url, \n                    \"-\", \n                    song_url, \n                    \"-lyrics\" \n                    )\n  \n  #Initialize web scrapping session\n  session &lt;- bow(full_url, force = TRUE)\n  \n    #Scrape the page\n    page &lt;- scrape(session)\n    if(is.null(page)){\n      warning(\"Invalid URL Entered: \", full_url, call. = T, immediate. = T)\n      tibble(\n        song,\n        artist, \n        lyrics = NA\n      )\n    }\n    else{\n    \n      #Change all &lt;br&gt; tags into spaces \n      #(why does html_text not do this automatically!?)\n      xml_find_all(page, \".//br\") |&gt; \n        xml_add_sibling(\"p\", \" \")\n      \n      xml_find_all(page, \".//br\") |&gt; \n        xml_remove()\n      \n      #Extract Lyrics\n      lyrics &lt;- html_nodes(page, \".Lyrics__Container-sc-1ynbvzw-1\") |&gt; \n        html_text() |&gt; \n        str_replace_all(\"\\\\[[^\\\\]]+\\\\]\", \" \") |&gt; \n        paste(collapse = \"\")\n      \n      #Create Output tibble\n      tibble(\n        song, \n        artist, \n        lyrics\n      )\n    }\n}\n\n\nget_billboard_hot_100 &lt;- function(){\n  #Initialing Webscraping Session\n  session &lt;- bow(\"https://www.billboard.com/charts/hot-100/\", force = TRUE)\n  \n  #Vector for placements on the Billboard Hot 100\n  placement &lt;- (1:100)\n  \n  #Getting song titles from Billboard Hot 100\n  song_title &lt;- scrape(session) |&gt; \n    html_nodes(\"li.o-chart-results-list__item &gt; h3#title-of-a-story\") |&gt;\n    html_text() |&gt; \n    str_trim() #Removing white space from html_text\n  \n  #Getting artist from Billboard Hot 100\n  artist &lt;- scrape(session) |&gt; \n    html_nodes(\"li.o-chart-results-list__item &gt; span.c-label.a-no-trucate\") |&gt; \n    html_text() |&gt; \n    str_trim() #Removing white space from html_text\n  \n  #Getting how many weeks a song has been on the Billboard Hot 100\n  weeks_on_chart &lt;- scrape(session) |&gt; \n    html_nodes(\"ul.lrv-a-unstyle-list.lrv-u-flex.lrv-u-height-100p li:nth-child(6) &gt; span.c-label\") |&gt; \n    html_text() |&gt; \n    str_trim() #Removing white space from html_text\n  \n  #Getting the Week that this chart is describing \n  week_of_chart &lt;- scrape(session) |&gt;\n    html_nodes(\"p.c-tagline.a-font-primary-medium-xs\") |&gt; \n    html_text() |&gt; \n    str_extract(\"Week of (\\\\D+ \\\\d+, \\\\d{4})\", 1) |&gt; #RegEx to extract the week\n    mdy()\n  \n  #Combining into one tibble\n  tibble(\n    placement, \n    song_title,\n    artist, \n    weeks_on_chart,\n    week_of_chart,\n  )\n}\n\n\n\nShow the code\ntop_100 &lt;- get_billboard_hot_100()\n\ntop_100_lyrics &lt;- map2(top_100$song_title[1:25],\n                       top_100$artist[1:25],\n                       \\(x, y) get_song_lyrics_genius(x, y)\n                       ) |&gt; \n  bind_rows()\n\n\n\n\nShow the code\nTop_100_Wordcount &lt;- SentimentAnalysis::analyzeSentiment(top_100_lyrics$lyrics) %&gt;% select(\"WordCount\")\nTop_100_Sentiment &lt;- SentimentAnalysis::analyzeSentiment(top_100_lyrics$lyrics) %&gt;% select(\"SentimentGI\")\n\n\n\n\nShow the code\ntop_100_lyrics$WordCount &lt;- Top_100_Wordcount\ntop_100_lyrics$SentimentGI &lt;- Top_100_Sentiment\n\n\n\n\nShow the code\nprint(top_100_lyrics)\n\n\n# A tibble: 25 × 5\n   song               artist   lyrics WordCount$WordCount SentimentGI$Sentimen…¹\n   &lt;chr&gt;              &lt;chr&gt;    &lt;chr&gt;                &lt;dbl&gt;                  &lt;dbl&gt;\n 1 Love Somebody      Morgan … \"  Ru…                 154                 0.221 \n 2 A Bar Song (Tipsy) Shabooz… \"  My…                 230                -0.1   \n 3 Birds Of A Feather Billie … \"    …                 144                 0.0417\n 4 Die With A Smile   Lady Ga… \"  (O…                 128                 0.0859\n 5 Espresso           Sabrina… \"  No…                 200                 0.135 \n 6 I Had Some Help    Post Ma… \"  Yo…                 158                 0.0127\n 7 Lose Control       Teddy S… \"  So…                 136                -0.0294\n 8 APT.               ROSE & … \"  채…                 215                 0.0744\n 9 Taste              Sabrina… \"  Oh…                 151                 0.245 \n10 Beautiful Things   Benson … \"  Fo…                 133                 0.105 \n# ℹ 15 more rows\n# ℹ abbreviated name: ¹​SentimentGI$SentimentGI"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "Mini_Project_4",
    "section": "",
    "text": "library(jsonlite)\nlibrary(tidyverse) \n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks jsonlite::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidytext)\nlibrary(textdata)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nlibrary(ggthemes)\nlibrary(gutenbergr)\njeopardy_data &lt;- fromJSON(\"~/Downloads/JEOPARDY_QUESTIONS1.json\", flatten = TRUE)\n\n\njeopardy_data %&gt;%\n    mutate(questionlength = str_count(question)) %&gt;% #Element\n  group_by(value) %&gt;%\n  mutate(mean_question_lenngth = mean(questionlength)) %&gt;% #Element\n  filter(value == \"$200\" | value == \"$400\" | value == \"$600\" |value == \"$800\" |value == \"$1000\") %&gt;% #Element\nslice_head(n = 1) %&gt;%\n  select(value, mean_question_lenngth) %&gt;%\n  ggplot(aes(x = reorder(value, mean_question_lenngth), y = mean_question_lenngth, fill = value)) +\n    geom_col() + \n  theme_bw() + \n  labs(title = \"Average character length of each common jeopardy value\", x = \"Value\", y = \"Average Question Length in characters\")\n\n\n\n\n\n\n\n\nThis first plot shows us an interesting trend amongst the most common jeopardy question values. The higher the value, the more characters the question contains\n\ngetyear &lt;- function(year){\nyear &lt;- paste(\"19\", year, \"[1-9]\", sep = \"\")\nlength(str_subset(jeopardy_data$question, year)) #Element\n}\n\n\nyearcount &lt;- lapply(c(0:9), getyear)\nyearcount &lt;- unlist(yearcount)\nyears &lt;- c(\"1900\", \"1910\", \"1920\", \"1930\", \"1940\", \"1950\", \"1960\", \"1970\", \"1980\", \"1990\")\ntibble(years, yearcount) %&gt;%\n  ggplot(aes(x = years, y = yearcount, color = years)) + \n  geom_point() +\n  theme_bw() +\n  labs(title = \"How many times was each decade in the 1900s mentioned in jeopardy questions\", x = \"year\", y = \"count\")\n\n\n\n\n\n\n\n\nThis plot shows us a map of how much each decade in the 1900s is mentioned in jeopardy questions. It very interesting to see an almost exponentially increasing of the mentions as the decades get later\n\nWho &lt;- length(str_subset(jeopardy_data$question, \"who\")) #Element\nWhat &lt;- length(str_subset(jeopardy_data$question, \"what\")) #Element\nWhere &lt;- length(str_subset(jeopardy_data$question, \"where\")) #Element\nWhen &lt;- length(str_subset(jeopardy_data$question, \"when\")) #Element\nWhy &lt;- length(str_subset(jeopardy_data$question, \"why\")) #Element\nHow &lt;- length(str_subset(jeopardy_data$question, \"how\")) #Element\n\n\ntibble(c(\"Who\", \"What\", \"Where\", \"When\", \"Why\", \"How\"), c(Who, What, Where, When, Why, How)) %&gt;%\n  rename(Phrase = `c(\"Who\", \"What\", \"Where\", \"When\", \"Why\", \"How\")`, \n         Num_Mentions = `c(Who, What, Where, When, Why, How)`)\n\n# A tibble: 6 × 2\n  Phrase Num_Mentions\n  &lt;chr&gt;         &lt;int&gt;\n1 Who            9769\n2 What           1947\n3 Where          2185\n4 When           4494\n5 Why             243\n6 How            4770\n\n\nIn order to ask a question you might have to use a common question word (Who, What, Where, When, Why, How). Here is how much those 6 words are used\n\ntidy_ngram &lt;- jeopardy_data |&gt;\n  unnest_tokens(bigram, question, token = \"ngrams\", n = 2) |&gt; #Element\n  filter(bigram != \"NA\")\n\nall_rows &lt;- tidy_ngram |&gt;\n  count(bigram, sort = TRUE)\n\nbad_rows &lt;- tidy_ngram |&gt;\n  count(bigram, sort = TRUE) %&gt;% slice(5:10) #Element\n\nall_rows %&gt;% anti_join(bad_rows) %&gt;% #Element\n  filter(bigram != \"jpg target\") %&gt;% \n  slice_head(n = 15) %&gt;%\n  ggplot(aes(x = bigram, y = n)) + \n  geom_col() +\n  theme_bw() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + \n  labs(title = \"Most common two word phrases in jeopardy questions\", x = \"Phrase\", y = \"Frequency\")\n\nJoining with `by = join_by(bigram, n)`\n\n\n\n\n\n\n\n\n\nSimilar to above, We again wanted to look at word counts. Now we have the most common two word phrases\n\nbing_sentiments &lt;- get_sentiments(lexicon = \"bing\") #Element\ntidy_questions &lt;- jeopardy_data |&gt;\n  mutate(line = row_number()) |&gt;\n  unnest_tokens(word, question, token = \"words\") #Element\n\ntidy_questions |&gt;   \n  inner_join(bing_sentiments) |&gt; \n  count(sentiment)\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(tidy_questions, bing_sentiments): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 375727 of `x` matches multiple rows in `y`.\nℹ Row 5711 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n  sentiment     n\n1  negative 57996\n2  positive 77667\n\n\nLastly we wanted to see if out of all these words we are looking at, do we have an overall positiv e or negative sentiment. Here with the totals we can see there are more positive words. A good way to put a positive note on the end of this project!"
  },
  {
    "objectID": "Untitled.html#jeopardy-questions",
    "href": "Untitled.html#jeopardy-questions",
    "title": "Mini_Project_4",
    "section": "",
    "text": "library(jsonlite)\nlibrary(tidyverse) \n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks jsonlite::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidytext)\nlibrary(textdata)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nlibrary(ggthemes)\nlibrary(gutenbergr)\njeopardy_data &lt;- fromJSON(\"~/Downloads/JEOPARDY_QUESTIONS1.json\", flatten = TRUE)\n\n\njeopardy_data %&gt;%\n    mutate(questionlength = str_count(question)) %&gt;% #Element\n  group_by(value) %&gt;%\n  mutate(mean_question_lenngth = mean(questionlength)) %&gt;% #Element\n  filter(value == \"$200\" | value == \"$400\" | value == \"$600\" |value == \"$800\" |value == \"$1000\") %&gt;% #Element\nslice_head(n = 1) %&gt;%\n  select(value, mean_question_lenngth) %&gt;%\n  ggplot(aes(x = reorder(value, mean_question_lenngth), y = mean_question_lenngth, fill = value)) +\n    geom_col() + \n  theme_bw() + \n  labs(title = \"Average character length of each common jeopardy value\", x = \"Value\", y = \"Average Question Length in characters\")\n\n\n\n\n\n\n\n\nThis first plot shows us an interesting trend amongst the most common jeopardy question values. The higher the value, the more characters the question contains\n\ngetyear &lt;- function(year){\nyear &lt;- paste(\"19\", year, \"[1-9]\", sep = \"\")\nlength(str_subset(jeopardy_data$question, year)) #Element\n}\n\n\nyearcount &lt;- lapply(c(0:9), getyear)\nyearcount &lt;- unlist(yearcount)\nyears &lt;- c(\"1900\", \"1910\", \"1920\", \"1930\", \"1940\", \"1950\", \"1960\", \"1970\", \"1980\", \"1990\")\ntibble(years, yearcount) %&gt;%\n  ggplot(aes(x = years, y = yearcount, color = years)) + \n  geom_point() +\n  theme_bw() +\n  labs(title = \"How many times was each decade in the 1900s mentioned in jeopardy questions\", x = \"year\", y = \"count\")\n\n\n\n\n\n\n\n\nThis plot shows us a map of how much each decade in the 1900s is mentioned in jeopardy questions. It very interesting to see an almost exponentially increasing of the mentions as the decades get later\n\nWho &lt;- length(str_subset(jeopardy_data$question, \"who\")) #Element\nWhat &lt;- length(str_subset(jeopardy_data$question, \"what\")) #Element\nWhere &lt;- length(str_subset(jeopardy_data$question, \"where\")) #Element\nWhen &lt;- length(str_subset(jeopardy_data$question, \"when\")) #Element\nWhy &lt;- length(str_subset(jeopardy_data$question, \"why\")) #Element\nHow &lt;- length(str_subset(jeopardy_data$question, \"how\")) #Element\n\n\ntibble(c(\"Who\", \"What\", \"Where\", \"When\", \"Why\", \"How\"), c(Who, What, Where, When, Why, How)) %&gt;%\n  rename(Phrase = `c(\"Who\", \"What\", \"Where\", \"When\", \"Why\", \"How\")`, \n         Num_Mentions = `c(Who, What, Where, When, Why, How)`)\n\n# A tibble: 6 × 2\n  Phrase Num_Mentions\n  &lt;chr&gt;         &lt;int&gt;\n1 Who            9769\n2 What           1947\n3 Where          2185\n4 When           4494\n5 Why             243\n6 How            4770\n\n\nIn order to ask a question you might have to use a common question word (Who, What, Where, When, Why, How). Here is how much those 6 words are used\n\ntidy_ngram &lt;- jeopardy_data |&gt;\n  unnest_tokens(bigram, question, token = \"ngrams\", n = 2) |&gt; #Element\n  filter(bigram != \"NA\")\n\nall_rows &lt;- tidy_ngram |&gt;\n  count(bigram, sort = TRUE)\n\nbad_rows &lt;- tidy_ngram |&gt;\n  count(bigram, sort = TRUE) %&gt;% slice(5:10) #Element\n\nall_rows %&gt;% anti_join(bad_rows) %&gt;% #Element\n  filter(bigram != \"jpg target\") %&gt;% \n  slice_head(n = 15) %&gt;%\n  ggplot(aes(x = bigram, y = n)) + \n  geom_col() +\n  theme_bw() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + \n  labs(title = \"Most common two word phrases in jeopardy questions\", x = \"Phrase\", y = \"Frequency\")\n\nJoining with `by = join_by(bigram, n)`\n\n\n\n\n\n\n\n\n\nSimilar to above, We again wanted to look at word counts. Now we have the most common two word phrases\n\nbing_sentiments &lt;- get_sentiments(lexicon = \"bing\") #Element\ntidy_questions &lt;- jeopardy_data |&gt;\n  mutate(line = row_number()) |&gt;\n  unnest_tokens(word, question, token = \"words\") #Element\n\ntidy_questions |&gt;   \n  inner_join(bing_sentiments) |&gt; \n  count(sentiment)\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(tidy_questions, bing_sentiments): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 375727 of `x` matches multiple rows in `y`.\nℹ Row 5711 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n  sentiment     n\n1  negative 57996\n2  positive 77667\n\n\nLastly we wanted to see if out of all these words we are looking at, do we have an overall positiv e or negative sentiment. Here with the totals we can see there are more positive words. A good way to put a positive note on the end of this project!"
  }
]